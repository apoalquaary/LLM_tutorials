{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb4eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "from torch import bfloat16\n",
    "import time\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94da652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77be14bc52954d0891032f137dc734aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config_4 = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model_id = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir='models/', quantization_config=bnb_config_4,  device_map={\"\": 0}) # cache_dir  \n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    pad_token_id = 50256,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe) #model_kwargs={\"temperature\":1e-10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bc3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please provide honest and straightforward answers to the following questions. Avoid providing misleading or inaccurate information. Your responses are expected to be truthful and directly address the questions asked.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dcdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=local_llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a33ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide honest and straightforward answers to the following questions. Avoid providing misleading or inaccurate information. Your responses are expected to be truthful and directly address the questions asked.\n",
      "\n",
      "Question: Write a short peom about rain\n",
      "Answer: Sure, here is a short poem about rain:\n",
      "Rain, oh rain, how you bring us cheer\n",
      "Washing away our fears and drying our tears\n",
      "With every drop, a new beginning starts\n",
      "A chance for growth, and a brand new start\n",
      "The smell of wet earth fills the air\n",
      "As we dance in the puddles with joy beyond compare\n",
      "Rain, oh rain, you're a gift from above\n",
      "Bringing life to the world, and all its love.\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3.71 s, sys: 184 ms, total: 3.9 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# full example\n",
    "query = \"Write a short peom about rain\"\n",
    "llm_response = chain.run(query)\n",
    "\n",
    "print(prompt_template.format(question=query) + llm_response)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80e50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide honest and straightforward answers to the following questions. Avoid providing misleading or inaccurate information. Your responses are expected to be truthful and directly address the questions asked.\n",
      "\n",
      "Question: Write a small story about kids\n",
      "Answer: Sure! Here is a small story about kids:\n",
      "Once upon a time, there were three kids named Jack, Jill, and Tom. They lived in a small town surrounded by rolling hills and green forests. Every day, they would play together in the woods, exploring new paths and discovering hidden streams. One sunny afternoon, while wandering deeper into the forest than ever before, they stumbled upon an old, mysterious-looking door hidden behind a thick cluster of bushes. Curious, they pushed the door open and found themselves in a magical garden filled with talking animals, colorful flowers, and a sparkling fountain. The kids played with the animals, picked wildflowers, and drank from the fountain until the sun began to set. As they left the garden, they promised to return the next day and explore more of its secrets. From that day on, they visited the magical garden every day after school, making new friends and having the most wonderful adventures.\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 7.01 s, sys: 31.3 ms, total: 7.04 s\n",
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# full example\n",
    "query = \"Write a small story about kids\"\n",
    "llm_response = chain.run(query)\n",
    "\n",
    "print(prompt_template.format(question=query) + llm_response)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26915817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
