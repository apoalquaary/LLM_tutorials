{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "from torch import bfloat16\n",
    "import time\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config_4 = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_id = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir='models/', quantization_config=bnb_config_4,  device_map={\"\": 0}) # cache_dir  \n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    pad_token_id = 50256,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe) #model_kwargs={\"temperature\":1e-10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please provide honest and straightforward answers to the following questions. Avoid providing misleading or inaccurate information. Your responses are expected to be truthful and directly address the questions asked.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=local_llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# full example\n",
    "query = \"Write a short peom about rain\"\n",
    "llm_response = chain.run(query)\n",
    "\n",
    "print(prompt_template.format(question=query) + llm_response)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e50be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
